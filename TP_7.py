# -*- coding: utf-8 -*-
"""TP7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mShVYsHNSdcy4oOsYZ8WLfxFV8ooVc9A
"""

# Importation des bibliothèques nécessaires
import os
import json
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt
from datasets import load_dataset, concatenate_datasets
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Chargement et fusion de deux jeux de données textuels avec uniformisation des labels

def load_and_prepare_datasets():
    ds1 = load_dataset("zeroshot/twitter-financial-news-sentiment", split="train")
    ds2 = load_dataset("nickmuchi/financial-classification", split="train")
    ds1 = ds1.rename_columns({"text": "text", "label": "labels"})
    ds2 = ds2.rename_columns({"text": "text", "labels": "labels"})
    combined = concatenate_datasets([ds1, ds2])
    return combined.shuffle(seed=42).select(range(600))

# Calcul des métriques d'évaluation pour la classification

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1, "precision": precision, "recall": recall}

# Entraînement d'un modèle BERT ou FinBERT sur un jeu de données textuel

def train_model(model_name, dataset, batch_size, num_epochs):
    tokenizer = BertTokenizer.from_pretrained(model_name)
    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)

    def tokenize(example):
        return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

    tokenized = dataset.train_test_split(test_size=0.3, seed=42)
    tokenized_train = tokenized["train"].map(tokenize, batched=True)
    tokenized_test = tokenized["test"].map(tokenize, batched=True)

    tokenized_train.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
    tokenized_test.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

    model_short = model_name.split("/")[-1]
    output_dir = f"./{model_short}_results"

    training_args = TrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        save_strategy="epoch",
        logging_strategy="epoch",
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=num_epochs,
        weight_decay=0.01,
        report_to="none",
        disable_tqdm=False,
        logging_first_step=True
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )

    trainer.train()
    eval_result = trainer.evaluate()

    print(f"\nRésultats d'évaluation pour {model_short.upper()} :")
    for k, v in eval_result.items():
        print(f"{k}: {v:.4f}")

    return model, tokenizer, tokenized, eval_result

# Prédictions sur un petit échantillon du jeu de test

def test_model_predictions(model, tokenizer, ds, label, n=5):
    model.eval()
    sample = ds["test"].select(range(n))
    texts = sample["text"]
    labels = sample["labels"]
    inputs = tokenizer(texts, truncation=True, padding="max_length", max_length=128, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = model(**inputs)
        preds = torch.argmax(outputs.logits, dim=-1).tolist()

    print(f"\nPrédictions test pour {label} :")
    for text, true_label, pred_label in zip(texts, labels, preds):
        print(f"\nTexte : {text[:100]}...")
        print(f"  Vrai label     : {['Négatif', 'Neutre', 'Positif'][true_label]}")
        print(f"  Prédiction {label}: {['Négatif', 'Neutre', 'Positif'][pred_label]}")

# Prédictions sur les fichiers JSON d'actualités d'entreprises

def predict_company_news(model, tokenizer, label, batch_size=32):
    data_folder = "Data"
    model.eval()

    for filename in os.listdir(data_folder):
        if filename.endswith("_news.json"):
            with open(os.path.join(data_folder, filename), "r") as f:
                raw = json.load(f)

            all_items = []
            if isinstance(raw, dict):
                for items in raw.values():
                    all_items.extend(items)
            else:
                all_items = raw

            texts = []
            for item in all_items:
                title = item.get("title", "")
                description = item.get("description", "")
                full_text = f"{title}. {description}" if description else title
                if full_text.strip():
                    texts.append(full_text)

            if not texts:
                print(f"\nAucun texte valide dans {filename}")
                continue

            predictions = []
            print(f"\nPrédictions ({label}) pour {filename} :")
            for i in tqdm(range(0, len(texts), batch_size), desc=filename, leave=False):
                batch_texts = texts[i:i+batch_size]
                inputs = tokenizer(batch_texts, truncation=True, padding="max_length", max_length=128, return_tensors="pt")
                inputs = {k: v.to(model.device) for k, v in inputs.items()}

                with torch.no_grad():
                    outputs = model(**inputs)
                    batch_preds = torch.argmax(outputs.logits, dim=-1).tolist()
                    predictions.extend(batch_preds)

            for text, label_id in zip(texts, predictions):
                sentiment = ["Négatif", "Neutre", "Positif"][label_id]
                print(f"- {sentiment}: {text[:100]}...")

# Entraînement, évaluation et comparaison graphique
if __name__ == "__main__":
    dataset = load_and_prepare_datasets()

    bert_model, bert_tokenizer, bert_ds, bert_metrics = train_model("bert-base-uncased", dataset, batch_size=8, num_epochs=3)
    finbert_model, finbert_tokenizer, finbert_ds, finbert_metrics = train_model("ProsusAI/finbert", dataset, batch_size=8, num_epochs=3)

    test_model_predictions(bert_model, bert_tokenizer, bert_ds, "BERT")
    predict_company_news(bert_model, bert_tokenizer, "BERT")
    test_model_predictions(finbert_model, finbert_tokenizer, finbert_ds, "FinBERT")
    predict_company_news(finbert_model, finbert_tokenizer, "FinBERT")

    metrics = ["accuracy", "f1", "precision", "recall"]
    bert_scores = [bert_metrics.get(m) or bert_metrics.get(f"eval_{m}") for m in metrics]
    finbert_scores = [finbert_metrics.get(m) or finbert_metrics.get(f"eval_{m}") for m in metrics]

    print("bert_metrics =", bert_metrics)
    print("finbert_metrics =", finbert_metrics)

    x = range(len(metrics))
    plt.figure(figsize=(8, 5))
    plt.bar([i - 0.2 for i in x], bert_scores, width=0.4, label="BERT")
    plt.bar([i + 0.2 for i in x], finbert_scores, width=0.4, label="FinBERT")
    plt.xticks(x, metrics)
    plt.ylim(0, 1)
    plt.ylabel("Score")
    plt.title("Comparaison des performances : BERT vs FinBERT")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.7)
    plt.tight_layout()
    folder_path = os.path.join("Résultats", "Prédiction_TP7")
    os.makedirs(folder_path, exist_ok=True)
    # Nom du fichier
    filename = f"TP7_Modèle.png"
    file_path = os.path.join(folder_path, filename)
    plt.savefig(file_path)
    plt.close()
