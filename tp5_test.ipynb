{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67beee73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_folder = \"Companies_historical_data\" \n",
    "output_folder = \"datasets\" \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "n_days = 30\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Lister tous les fichiers CSV\n",
    "file_list = [f for f in os.listdir(data_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Mod√®les √† tester\n",
    "model_types = [\"MLP\", \"RNN\", \"LSTM\"]\n",
    "\n",
    "# Fichiers dans le dossier datasets\n",
    "file_list = sorted([f for f in os.listdir(\"datasets\") if f.endswith(\"_x_train.npy\")])\n",
    "entreprises = [f.split(\"_\")[0] for f in file_list]\n",
    "\n",
    "## ---- PARTIE 1 ----\n",
    "# Chargement des prix close\n",
    "def load_close_prices(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df[['Close']]\n",
    "\n",
    "# Standardisation + split\n",
    "def scale_and_split(data, split_ratio=0.8):\n",
    "    scaler = MinMaxScaler()\n",
    "    train_size = int(len(data) * split_ratio)\n",
    "    train_data = data[:train_size]\n",
    "    test_data = data[train_size:]\n",
    "    train_scaled = scaler.fit_transform(train_data)\n",
    "    test_scaled = scaler.transform(test_data)\n",
    "    return train_scaled, test_scaled, scaler\n",
    "\n",
    "# Cr√©ation X et Y\n",
    "def create_target_features(df_scaled, n_days=30):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(n_days, len(df_scaled)):\n",
    "        x.append(df_scaled[i - n_days:i, 0])\n",
    "        y.append(df_scaled[i, 0])\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "# Pipeline complet pour un fichier\n",
    "def prepare_dataset(file_path, n_days=30, split_ratio=0.8):\n",
    "    df = load_close_prices(file_path)\n",
    "    train_scaled, test_scaled, scaler = scale_and_split(df, split_ratio)\n",
    "    x_train, y_train = create_target_features(train_scaled, n_days)\n",
    "    x_test, y_test = create_target_features(test_scaled, n_days)\n",
    "    return x_train, y_train, x_test, y_test, scaler\n",
    "\n",
    "def fonction_df():\n",
    "    # Pour chaque fichier, on pr√©pare et sauvegarde les donn√©es\n",
    "    for filename in file_list:\n",
    "        name = os.path.splitext(filename)[0]  # \"apple.csv\" -> \"apple\"\n",
    "        file_path = os.path.join(data_folder, filename)\n",
    "\n",
    "        # Pr√©paration des donn√©es (fonction que tu as d√©j√†)\n",
    "        x_train, y_train, x_test, y_test, scaler = prepare_dataset(file_path, n_days, split_ratio)\n",
    "\n",
    "        # Sauvegarde sous forme de fichiers .npy et .pkl\n",
    "        np.save(os.path.join(output_folder, f\"{name}_x_train.npy\"), x_train)\n",
    "        np.save(os.path.join(output_folder, f\"{name}_y_train.npy\"), y_train)\n",
    "        np.save(os.path.join(output_folder, f\"{name}_x_test.npy\"), x_test)\n",
    "        np.save(os.path.join(output_folder, f\"{name}_y_test.npy\"), y_test)\n",
    "        joblib.dump(scaler, os.path.join(output_folder, f\"{name}_scaler.pkl\"))\n",
    "\n",
    "        print(f\"{name} : fichiers sauvegard√©s.\")\n",
    "   \n",
    "fonction_df()\n",
    "     \n",
    "def load_dataset(name, folder=\"datasets\"):\n",
    "    x_train = np.load(f\"{folder}/{name}_x_train.npy\", allow_pickle=True)\n",
    "    y_train = np.load(f\"{folder}/{name}_y_train.npy\", allow_pickle=True)\n",
    "    x_test = np.load(f\"{folder}/{name}_x_test.npy\", allow_pickle=True)\n",
    "    y_test = np.load(f\"{folder}/{name}_y_test.npy\", allow_pickle=True)\n",
    "    scaler = joblib.load(f\"{folder}/{name}_scaler.pkl\")\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "    return x_train, y_train, x_test, y_test, scaler\n",
    "\n",
    "## --- PARTIE 2 ---\n",
    "def build_mlp_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=input_shape))\n",
    "    for dim in hidden_dims:\n",
    "        model.add(tf.keras.layers.Dense(dim, activation=activation))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    opt = getattr(tf.keras.optimizers, optimizer)(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def build_rnn_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    for i, dim in enumerate(hidden_dims):\n",
    "        return_seq = i < len(hidden_dims) - 1\n",
    "        model.add(tf.keras.layers.SimpleRNN(dim, activation=activation, return_sequences=return_seq, input_shape=input_shape if i == 0 else None))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    opt = getattr(tf.keras.optimizers, optimizer)(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def build_lstm_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate):\n",
    "    model = tf.keras.Sequential()\n",
    "    for i, dim in enumerate(hidden_dims):\n",
    "        return_seq = i < len(hidden_dims) - 1\n",
    "        model.add(tf.keras.layers.LSTM(dim, activation=activation, return_sequences=return_seq, input_shape=input_shape if i == 0 else None))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    opt = getattr(tf.keras.optimizers, optimizer)(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "def train_model(model_type, X_train, y_train, input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate, epochs, batch_size):\n",
    "    if model_type == \"MLP\":\n",
    "        model = build_mlp_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate)\n",
    "    elif model_type == \"RNN\":\n",
    "        model = build_rnn_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate)\n",
    "    elif model_type == \"LSTM\":\n",
    "        model = build_lstm_model(input_shape, hidden_dims, activation, dropout_rate, optimizer, learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Mod√®le non reconnu.\")\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    return model\n",
    "\n",
    "def predict(model, X_test, y_test, scaler, model_type, entreprise_name=None):\n",
    "    # 1. Pr√©diction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 2. Inversion du scaling\n",
    "    y_pred_inv = scaler.inverse_transform(y_pred.reshape(-1, 1))\n",
    "    y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # 3. √âvaluation\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "    print(f\"üîç {entreprise_name or ''} - {model_type} : MAE = {mae:.4f}, RMSE = {rmse:.4f}\")\n",
    "\n",
    "    # 4. Affichage des 10 premi√®res pr√©dictions vs vraies valeurs\n",
    "    print(\"Pr√©dictions vs R√©el (10 premi√®res valeurs) :\")\n",
    "    for i in range(min(10, len(y_test_inv))):\n",
    "        print(f\"R√©el : {y_test_inv[i][0]:.2f} \\t Pr√©dit : {y_pred_inv[i][0]:.2f}\")\n",
    "\n",
    "    # 5. Courbe r√©elle vs pr√©dite (ex: 50 premiers points)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test_inv[:50], label=\"R√©el\", marker='o')\n",
    "    plt.plot(y_pred_inv[:50], label=\"Pr√©vu\", marker='x')\n",
    "    plt.title(f\"{model_type} - Pr√©diction vs R√©el ({entreprise_name or 'Entreprise'})\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Prix\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "## ---- PARTIE 3 ----\n",
    "def fonction_compar_entreprise():\n",
    "    for entreprise in entreprises:\n",
    "        print(f\"Traitement de l‚Äôentreprise : {entreprise.upper()}\")\n",
    "\n",
    "        # Charger les donn√©es\n",
    "        x_train, y_train, x_test, y_test, scaler = load_dataset(entreprise)\n",
    "\n",
    "        for model_type in model_types:\n",
    "            print(f\"\\n--- Mod√®le : {model_type} ---\")\n",
    "\n",
    "            # Entra√Ænement\n",
    "            model = train_model(\n",
    "                model_type=model_type,\n",
    "                X_train=x_train,\n",
    "                y_train=y_train,\n",
    "                input_shape=(x_train.shape[1], 1),\n",
    "                hidden_dims=[50],\n",
    "                activation=\"tanh\" if model_type != \"MLP\" else \"relu\",\n",
    "                dropout_rate=0.2,\n",
    "                optimizer=\"Adam\",\n",
    "                learning_rate=0.001,\n",
    "                epochs=20,\n",
    "                batch_size=32\n",
    "            )\n",
    "\n",
    "            # Pr√©diction + Affichage\n",
    "            predict(model, x_test, y_test, scaler, model_type=model_type, entreprise_name=entreprise)\n",
    "fonction_compar_entreprise()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
